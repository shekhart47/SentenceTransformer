Computing triplet lengths (num_proc=78):  99%|█████████▉| 50791297/51119000 [01:19<00:00, 640452.68 examples/s] 
---------------------------------------------------------------------------
RemoteTraceback                           Traceback (most recent call last)
RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 688, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3525, in _map_single
    for i, batch in iter_outputs(shard_iterable):
  File "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3475, in iter_outputs
    yield i, apply_function(example, i, offset=offset)
  File "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3398, in apply_function
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/tmp/ipykernel_7729/2913473554.py", line 70, in add_triplet_length
    out = tokenizer(
  File "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2855, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2943, in _call_one
    return self.batch_encode_plus(
  File "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3144, in batch_encode_plus
    return self._batch_encode_plus(
  File "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 553, in _batch_encode_plus
    encodings = self._tokenizer.encode_batch(
TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]
"""

The above exception was the direct cause of the following exception:

TypeError                                 Traceback (most recent call last)
Cell In[22], line 1
----> 1 train_dataset = build_huggingface_dataset()

Cell In[21], line 95, in build_huggingface_dataset()
     92 BATCHED_SIZE = 8192  # try 8k, 16k, 32k
     93 NUM_PROC = max(2, os.cpu_count() - 2)
---> 95 train_dataset = train_dataset.map(
     96     add_triplet_length,
     97     batched=True,
     98     batch_size=BATCHED_SIZE,
     99     num_proc=NUM_PROC,
    100     remove_columns=[],          # don't drop anything
    101     desc="Computing triplet lengths"
    102 )
    104 return train_dataset

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/datasets/arrow_dataset.py:557, in transmit_format.<locals>.wrapper(*args, **kwargs)
    550 self_format = {
    551     "type": self._format_type,
    552     "format_kwargs": self._format_kwargs,
    553     "columns": self._format_columns,
    554     "output_all_columns": self._output_all_columns,
    555 }
    556 # apply actual function
--> 557 out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
    558 datasets: list["Dataset"] = list(out.values()) if isinstance(out, dict) else [out]
    559 # re-apply format to the output

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/datasets/arrow_dataset.py:3171, in Dataset.map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)
   3165 logger.info(f"Spawning {num_proc} processes")
   3166 with hf_tqdm(
   3167     unit=" examples",
   3168     total=pbar_total,
   3169     desc=(desc or "Map") + f" (num_proc={num_proc})",
   3170 ) as pbar:
-> 3171     for rank, done, content in iflatmap_unordered(
   3172         pool, Dataset._map_single, kwargs_iterable=kwargs_per_job
   3173     ):
   3174         if done:
   3175             shards_done += 1

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/datasets/utils/py_utils.py:728, in iflatmap_unordered(pool, func, kwargs_iterable)
    725 finally:
    726     if not pool_changed:
    727         # we get the result in case there's an error to raise
--> 728         [async_result.get(timeout=0.05) for async_result in async_results]

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/datasets/utils/py_utils.py:728, in <listcomp>(.0)
    725 finally:
    726     if not pool_changed:
    727         # we get the result in case there's an error to raise
--> 728         [async_result.get(timeout=0.05) for async_result in async_results]

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/multiprocess/pool.py:774, in ApplyResult.get(self, timeout)
    772     return self._value
    773 else:
--> 774     raise self._value

TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]






---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
Cell In[23], line 1
----> 1 train_dataset[0]

NameError: name 'train_dataset' is not defined
